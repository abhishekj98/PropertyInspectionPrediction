
#feature selection to avoid overfitting
#after this by the importance data I got I decided to drop T2_V10, T2_V7,T1_V13,T1_V10


import numpy as np
from sklearn.externals import joblib

feature_labels= np.array(['Id','Hazard','T1_V1','T1_V2','T1_V3','T1_V4','T1_V5','T1_V6','T1_V7','T1_V8','T1_V9','T1_V10','T1_V11','T1_V12','T1_V13','T1_V14','T1_V15','T1_V16','T1_V17','T2_V1','T2_V2','T2_V3','T2_V4','T2_V5','T2_V6','T2_V7','T2_V8','T2_V9','T2_V10','T2_V11','T2_V12','T2_V13','T2_V14','T2_V15'])
model =joblib.load('trained_house_classifier_model.pkl')

importance= model.feature_importances_
feature_indexes_by_importance=importance.argsort()
print("\n\n The Percentage importance of the features after one hot ending will be.....")
for index in feature_indexes_by_importance:
    
    print(importance[index] *100)